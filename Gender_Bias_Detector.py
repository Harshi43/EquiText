# -*- coding: utf-8 -*-
"""Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TvmX5_YWxNgNMKGfofdq49aLRM_K9aUw

**Gender Bias Analysis Tool**

We developed a tool to analyze and detect gender bias in textbook texts, aiming to promote equitable representation in educational content. The tool uses basic AI, machine learning, and Python-based methods to assess gender visibility, contextual tone, and role diversity.

What We Did and Why

1. Text Preprocessing: Cleaned and standardized the text to focus on meaningful content. This ensures accurate analysis of gendered terms and context.


2. Gendered Word Analysis: Counted male and female terms to evaluate visibility and identify gender representation disparities.


3. Context Extraction: Analyzed sentences containing gendered words to understand how each gender is portrayed in context.


4. Sentiment Analysis: Measured the tone (positive, negative, or neutral) of sentences to identify biases in emotional portrayal.


5. Role Categorization: Extracted and categorized professions or roles linked to each gender to detect stereotypes.



Why It Matters

This tool empowers educators and policymakers to identify and address unconscious biases in educational materials, fostering a more inclusive learning environment. It provides data-driven insights to challenge stereotypes and ensure balanced gender representation in textbooks.

Install Libraries
"""

# Install necessary libraries
!pip install nltk spacy pandas matplotlib transformers pdfplumber
# Download SpaCy's English model
!python -m spacy download en_core_web_sm

"""step 2: Code: Load Text Data"""

import pdfplumber
import re
from google.colab import files

def extract_ncert_text(pdf_files):
    """
    Extract meaningful text from uploaded NCERT PDF files.
    :param pdf_files: Uploaded PDF files.
    :return: Dictionary with structured text content.
    """
    extracted_content = {}

    # Patterns to clean and categorize text
    exclude_patterns = [
        r'^(\d+\s*$|Chapter|Â©\s*NCERT|Fig\.|Table)',  # Page numbers, headers
        r'Activity\s*\d+(\.\d+)?',  # Activity markers
        r'\+\s$',  # Footnote markers
    ]
    section_patterns = {
        'main_text': [r'^[A-Z][a-z]', r'^(We|In|Since|Can|Do|For|What)'],
        'side_notes': [r'Activity', r'Questions', r'Note:', r'Hint:']
    }

    def is_valid_text_block(text):
        """
        Check if a text block is valid based on length and patterns.
        """
        text = text.strip()
        if len(text) < 30:
            return False
        for pattern in exclude_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return False
        return True

    def categorize_text_block(text):
        """
        Categorize text into main_text, side_notes, or misc.
        """
        for pattern in section_patterns['side_notes']:
            if re.search(pattern, text, re.IGNORECASE):
                return 'side_notes'
        for pattern in section_patterns['main_text']:
            if re.search(pattern, text):
                return 'main_text'
        return 'misc'

    # Process each uploaded PDF
    for file_name, file_content in pdf_files.items():
        extracted_content[file_name] = {'main_text': [], 'side_notes': [], 'misc': []}

        with open(file_name, 'wb') as f:
            f.write(file_content)

        with pdfplumber.open(file_name) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if not page_text:
                    continue
                # Split text into blocks
                text_blocks = re.split(r'\n{2,}', page_text)
                for block in text_blocks:
                    block = block.strip()
                    if is_valid_text_block(block):
                        category = categorize_text_block(block)
                        extracted_content[file_name][category].append(block)

    return extracted_content

# Step 2 Execution
uploaded_files = files.upload()
extracted_text = extract_ncert_text(uploaded_files)

# Save extracted text for verification
for file_name, content in extracted_text.items():
    with open(f"{file_name}_extracted.txt", 'w', encoding='utf-8') as f:
        for category, blocks in content.items():
            f.write(f"\n\n--- {category.upper()} ---\n\n")
            f.write("\n\n".join(blocks))

print("Text extraction completed and saved.")

"""step 3: Code: Preprocess Text"""

import re

def preprocess_text_block(block, retain_punctuation=False):
    """
    Clean and preprocess text block:
    - Optionally retain punctuation.
    - Remove extra spaces, convert to lowercase.
    """
    block = re.sub(r'\s+', ' ', block)  # Remove extra spaces
    if not retain_punctuation:
        block = re.sub(r'[^\w\s]', '', block)  # Remove punctuation
    block = block.lower()  # Convert to lowercase
    return block

def preprocess_extracted_content(extracted_content):
    """
    Preprocess extracted content:
    - Create dual streams: with punctuation and without punctuation.
    """
    preprocessed_content = {}
    for file_name, sections in extracted_content.items():
        preprocessed_content[file_name] = {
            section: {
                "retain_punctuation": [preprocess_text_block(text, retain_punctuation=True) for text in texts],
                "no_punctuation": [preprocess_text_block(text, retain_punctuation=False) for text in texts]
            }
            for section, texts in sections.items()
        }
    return preprocessed_content

# Step 3 Execution
preprocessed_data = preprocess_extracted_content(extracted_text)

# Example Output for Verification
print("Preprocessing completed. Sample output:\n")
for file_name, sections in preprocessed_data.items():
    print(f"File: {file_name}")
    for section, streams in sections.items():
        print(f"\n  Section: {section}")
        print(f"    Sample with punctuation: {streams['retain_punctuation'][:1]}")
        print(f"    Sample without punctuation: {streams['no_punctuation'][:1]}")

"""step 4: Code: Count Gendered Words"""

from collections import Counter

# Comprehensive list of gendered words
gender_words = {
    "male": [
        "he", "him", "his", "himself", "man", "men", "boy", "boys", "father",
        "brother", "sir", "uncle", "uncles", "grandfather", "grandpa",
        "dad", "husband", "king", "prince", "lord", "master", "actor", "emperor",
        "son", "sons", "mr", "nephew", "policeman", "sportsman", "businessman",
        "chairman", "postman", "guru", "sahib", "zamindar", "ruler",
        "maharaja", "landlord", "pandit"
    ],
    "female": [
        "she", "her", "hers", "herself", "woman", "women", "girl", "girls", "mother",
        "mothers", "sister", "sisters", "madam", "ms", "mrs", "queen", "princess",
        "lady", "maid", "actress", "empress", "aunt", "grandmother", "grandma", "mom",
        "wife", "daughter", "devi", "maharani", "landlady",
        "widow", "goddess", "niece", "matriarch"
    ]
}


def count_gender_words_with_examples(text, gender_words):
    """
    Count occurrences of gendered words in the text using exact matches.
    Return both counts and examples found in the text.
    """
    gender_count = {}
    gender_examples = {}

    for gender, words in gender_words.items():
        word_counts = {}
        examples = set()

        for word in words:
            matches = re.findall(rf'\b{word}\b', text, re.IGNORECASE)
            count = len(matches)
            if count > 0:
                word_counts[word] = count
                examples.update(matches)

        gender_count[gender] = sum(word_counts.values())
        gender_examples[gender] = list(examples)  # Convert set to list for compatibility

    return gender_count, gender_examples

# Perform gendered word frequency analysis
gender_analysis = {}

for file, sections in preprocessed_data.items():
    combined_text = []
    for section, streams in sections.items():
        combined_text.extend(streams["no_punctuation"])

    full_text = " ".join(combined_text)
    counts, examples = count_gender_words_with_examples(full_text, gender_words)
    gender_analysis[file] = {"counts": counts, "examples": examples}

# Display results for each file
print("Gendered Word Frequency Analysis:\n")
for file, analysis in gender_analysis.items():
    print(f"File: {file}")
    for gender, count in analysis["counts"].items():
        print(f"  {gender.capitalize()} words: {count}")
        found_words = analysis["examples"][gender][:5]
        print(f"    Examples found: {', '.join(found_words) if found_words else 'None'}")
    print()

"""Code: Extract Context Sentences

step 5: Code: Load SpaCy and Extract Context
"""

import spacy

# Load SpaCy's English model
nlp = spacy.load("en_core_web_sm")

def extract_context_sentences(text_blocks, gender_words):
    """
    Extract sentences containing gendered words using SpaCy.

    :param text_blocks: List of text blocks to analyze.
    :param gender_words: Dictionary of gendered words.
    :return: Dictionary with context sentences categorized by gender.
    """
    # Initialize dictionary for context sentences
    context_sentences = {gender: set() for gender in gender_words.keys()}

    # Iterate through each block and extract sentences with gendered words
    for block in text_blocks:
        # Process block using SpaCy NLP pipeline
        doc = nlp(block)
        for sent in doc.sents:
            # Extract tokens from the sentence
            tokens = {token.text.lower() for token in sent if token.is_alpha}

            # Check each gender category
            for gender, words in gender_words.items():
                if any(word in tokens for word in words):
                    context_sentences[gender].add(sent.text.strip())

    # Convert sets back to lists to return clean results
    return {gender: list(sentences) for gender, sentences in context_sentences.items()}

# Example Execution for Step 5
context_analysis = {
    file: {
        section: extract_context_sentences(data["retain_punctuation"], gender_words)
        for section, data in sections.items()
    }
    for file, sections in preprocessed_data.items()
}

# Sample Output for Step 5
print("Context Extraction Completed. Sample output:")
for file, sections in context_analysis.items():
    print(f"\nFile: {file}")
    for section, gender_context in sections.items():
        print(f"  Section: {section}")
        for gender, sentences in gender_context.items():
            print(f"    {gender.capitalize()} Sentences ({len(sentences)}):")
            print("      ", sentences[:3])  # Display up to 3 sample sentences

"""step 6: Setup for Sentiment Analysis"""

from transformers import pipeline, AutoTokenizer

# Load Hugging Face sentiment analysis pipeline and tokenizer
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
sentiment_pipeline = pipeline("sentiment-analysis", model=model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_sentences(sentences, max_tokens=512):
    """
    Preprocess sentences for sentiment analysis:
    - Truncate sentences to fit the tokenizer's max tokens.

    :param sentences: List of sentences to preprocess.
    :param max_tokens: Maximum number of tokens for each sentence.
    :return: List of processed sentences.
    """
    processed = []
    for sentence in sentences:
        # Tokenize and truncate sentences
        encoded = tokenizer(sentence, truncation=True, max_length=max_tokens, return_tensors="pt")
        processed_text = tokenizer.decode(encoded["input_ids"][0], skip_special_tokens=True)
        processed.append(processed_text)
    return processed


def analyze_sentiments(context_analysis, max_tokens=512, batch_size=8):
    """
    Perform sentiment analysis on context sentences.

    :param context_analysis: Dictionary of sentences categorized by gender.
    :param max_tokens: Maximum tokens per sentence for the tokenizer.
    :param batch_size: Batch size for sentiment analysis.
    :return: Dictionary of sentiment analysis results.
    """
    sentiment_results = {}

    # Iterate through each file and section
    for file, sections in context_analysis.items():
        sentiment_results[file] = {}
        for section, gender_contexts in sections.items():
            sentiment_results[file][section] = {}
            for gender, sentences in gender_contexts.items():
                # Preprocess sentences
                processed_sentences = preprocess_sentences(sentences, max_tokens)
                sentiments = []

                # Perform sentiment analysis in batches
                for i in range(0, len(processed_sentences), batch_size):
                    batch = processed_sentences[i:i + batch_size]
                    batch_results = sentiment_pipeline(batch)
                    sentiments.extend(batch_results)

                # Save sentiment results
                sentiment_results[file][section][gender] = [
                    {"sentence": sentence, "sentiment": sentiment["label"], "confidence": sentiment["score"]}
                    for sentence, sentiment in zip(processed_sentences, sentiments)
                ]

    return sentiment_results

# Example Execution for Step 6
sentiment_analysis = analyze_sentiments(context_analysis)

# Sample Output for Step 6
print("Sentiment Analysis Completed. Sample output:")
for file, sections in sentiment_analysis.items():
    print(f"\nFile: {file}")
    for section, gender_sentiments in sections.items():
        print(f"  Section: {section}")
        for gender, analysis in gender_sentiments.items():
            print(f"    {gender.capitalize()} Sentiments ({len(analysis)}):")
            for entry in analysis[:3]:  # Display up to 3 examples
                print(f"      Sentence: {entry['sentence']}")
                print(f"        Sentiment: {entry['sentiment']} ({entry['confidence']:.2f})")

"""step 7: Code: Extract and Categorize Roles"""

# Define a list of common roles/professions
roles_list = [
    "actor", "actress", "administrator", "agriculturist", "architect", "artist",
    "astronomer", "attendant", "ayurvedic doctor", "baker", "barber", "biologist",
    "blacksmith", "builder", "butler", "buyer", "carpenter", "cashier", "chef",
    "chemist", "cleaner", "clerk", "consultant", "counsellor", "craftsperson",
    "dealer", "doctor", "driver", "economist", "editor", "engineer", "envoy",
    "executive", "farmer", "fisherman", "florist", "foreman", "gardener",
    "governor", "goldsmith", "hawker", "historian", "housekeeper", "housewife",
    "illustrator", "industrialist", "instructor", "inspector", "interpreter",
    "inventor", "journalist", "judge", "kabadiwala", "labourer", "landlord",
    "librarian", "magician", "maid", "magistrate", "manager", "mathematician",
    "mechanic", "merchant", "messenger", "miller", "musician", "nanny", "nurse",
    "operator", "pandit", "physician", "physicist", "photographer", "pilot",
    "player", "porter", "potter", "priest", "producer", "proprietor",
    "publisher", "ragpicker", "receptionist", "reporter", "researcher",
    "rickshaw puller", "sculptor", "scientist", "secretary", "shepherd",
    "sportsman", "soldier", "steward", "street vendor", "supervisor", "surgeon",
    "surveyor", "tailor", "teacher", "textile worker", "translator",
    "treasurer", "tutor", "vaidya", "vendor", "waiter", "weaver", "writer"
]

def extract_roles(context_analysis, roles_list):
    """
    Extract roles and professions associated with gendered terms from context sentences.

    Parameters:
    - context_analysis: Dictionary of sentences categorized by gender and file.
    - roles_list: List of predefined roles/professions to search for.

    Returns:
    - roles_by_gender: Dictionary mapping files and genders to extracted roles and sentences.
    """
    # Initialize dictionary to store roles
    roles_by_gender = {file: {gender: [] for gender in gender_words.keys()} for file in context_analysis.keys()}

    # Iterate through files and gender contexts
    for file, gender_contexts in context_analysis.items():
        for gender, sentences in gender_contexts.items():
            for sent in sentences:
                doc = nlp(sent)  # Parse sentence using SpaCy
                for token in doc:
                    # Check if token matches any role/profession in roles_list
                    if token.text.lower() in roles_list:
                        roles_by_gender[file][gender].append((token.text.lower(), sent))  # Add role and sentence

    return roles_by_gender


def add_role_examples(roles_by_gender):
    """
    Generate example sentences for each role extracted.

    Parameters:
    - roles_by_gender: Dictionary of roles mapped to gender and sentences.

    Returns:
    - examples_by_role: Dictionary mapping roles to example sentences for each gender.
    """
    examples_by_role = {}
    for file, gender_roles in roles_by_gender.items():
        for gender, roles in gender_roles.items():
            # Extract unique roles and their sentences
            unique_roles = set(role for role, _ in roles)
            for role in unique_roles:
                examples_by_role.setdefault(role, {"male": [], "female": []})
                # Collect up to 2 example sentences per role and gender
                examples_by_role[role][gender].extend(
                    [sent for r, sent in roles if r == role][:2]
                )

    return examples_by_role


# Perform role extraction
roles_analysis = extract_roles(context_analysis, roles_list)

# Generate role examples
role_examples = add_role_examples(roles_analysis)

# Display extracted roles and example sentences
print("\nRole/Profession Extraction with Examples (Sample):\n")
for file, roles in roles_analysis.items():
    print(f"File: {file}")
    for gender, roles_list in roles.items():
        print(f"\n  {gender.capitalize()} Roles:")
        if roles_list:
            unique_roles = set(role for role, _ in roles_list)
            print(f"    Roles Identified: {', '.join(unique_roles)}")
            print("\n    Example Sentences:")
            for role in unique_roles:
                examples = role_examples[role][gender][:2]  # Limit to 2 examples
                if examples:
                    print(f"      {role.capitalize()}:")
                    for example in examples:
                        print(f"        - {example}")
                else:
                    print(f"      {role.capitalize()}: No examples found.")
        else:
            print(f"    No roles identified for {gender}.")
    print()


# Explanation:
# - Each role includes up to 2 example sentences where it was identified.
# - Examples are displayed under their respective gender and role for clarity.

# How Results are Calculated:
# 1. `extract_roles` identifies roles and the sentences in which they appear.
# 2. `add_role_examples` organizes roles with corresponding examples by gender.

# How to Interpret:
# - Use the examples to verify context or detect stereotypes.
#   For example:
#   - Check if "nurse" sentences reflect caregiving roles for females.
#   - Check if "scientist" is skewed toward male mentions.
# - Patterns in roles and examples can help in identifying implicit gender biases.